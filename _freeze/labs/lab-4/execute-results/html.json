{
  "hash": "8da5d9d022e62d177daea7d6fc913095",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 4 - Web scraping and ethics\"\ncategories: \"Lab\"\n---\n\n\n\n\n# Introduction\n\nIn this lab you'll build the data wrangling and visualization skills you've developed so far and data tidying and joining to your repertoire.\n\n::: callout-note\nThis lab assumes you've completed the labs so far and doesn't repeat setup and overview content from those labs.\nIf you have not yet done those, you should go back and review the previous labs before starting on this one.\n:::\n\n## Learning objectives\n\nBy the end of the lab, you will...\n\n-   Be able to scrape data from web pages using the **rvest** package\n-   Clean and analyze the data you have scraped\n-   Come up with your own questions about the data and answer them\n-   Be able to use the **robotstxt** package as well as read the Terms of Use/Service of a website to check if a website allows scraping\n-   Read articles on the ethics of AI tools and LLMs and reflect on the implications of these tools for society\n\n## Getting started\n\nLog in to RStudio, clone your `lab-4` repo from GitHub, open your `lab-4.qmd` document, and get started!\n\n::: {.callout-tip collapse=\"true\"}\n## Click here if you prefer to see step-by-step instructions\n\n### Log in to RStudio\n\n-   Go to <https://cmgr.oit.duke.edu/containers> and log in with your Duke NetID and Password.\n-   Click `STA198-199` under My reservations to log into your container. You should now see the RStudio environment.\n\n### Clone the repo & start new RStudio project\n\n-   Go to the course organization at [github.com/sta199-s24](https://github.com/sta199-s24) organization on GitHub.\n    Click on the repo with the prefix **lab-3**.\n    It contains the starter documents you need to complete the lab.\n\n-   Click on the green **CODE** button, select **Use SSH** (this might already be selected by default, and if it is, you'll see the text **Clone with SSH**).\n    Click on the clipboard icon to copy the repo URL.\n\n-   In RStudio, go to *File* ➛ *New Project* ➛*Version Control* ➛ *Git*.\n\n-   Copy and paste the URL of your assignment repo into the dialog box *Repository URL*. Again, please make sure to have *SSH* highlighted under *Clone* when you copy the address.\n\n-   Click *Create Project*, and the files from your GitHub repo will be displayed in the *Files* pane in RStudio.\n\n-   Click *lab-3.qmd* to open the template Quarto file.\n    This is where you will write up your code and narrative for the lab.\n\n### First steps\n\nIn `lab-3.qmd`, update the `author` field to your name, render your document and examine the changes.\nThen, in the Git pane, click on **Diff** to view your changes, add a commit message (e.g., \"Added author name\"), and click **Commit**.\nThen, push the changes to your GitHub repository, and in your browser confirm that these changes have indeed propagated to your repository.\n:::\n\n## Packages\n\nIn this lab we will work with the following packages:\n\n-   **tidyverse**: for tidy data wrangling and visualization\n-   **rvest**: for data scraping\n-   **robotstxt**: for checking the robots.txt files of websites for permission to scrape data\n-   **knitr:** for making a pretty table\n\nSome of the later questions in the lab ask you to come up with your own questions about the data and answer them.\nIf this requires additional packages, you should feel free to load them as well.\n\n## Guidelines\n\nAs we've discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\nIn addition, the code should all the code should be be able to be read (not run off the page) when you render to PDF.\nMake sure that is the case, and add line breaks where the code is running off the page.[^1]\n\n[^1]: Remember, haikus not novellas when writing code!\n\n::: callout-note\nContinuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course.\nThere will be periodic reminders in this assignment to remind you to **render, commit, and push** your changes to GitHub.\nYou should have at least 3 commits with meaningful commit messages by the end of the assignment.\n:::\n\n# Questions\n\n## Part 1 - Scraping data from web pages\n\n### Question 1\n\n::: callout-important\nYou do not need to do anything in your Quarto file for this question.\nAll of the work for this question will be done in `lab-4-clubs-scrape.R`.\n:::\n\nYour task for this question is to scrape data on Duke Student Groups from <https://dukegroups.com/club_signup?view=all>.\nScroll down and click on load all groups.\n\nYou will do this in `lab-4-clubs-scrape.R`, which gives some scaffolding for you to complete this task.\nThe goal is to scrape information on\n\n-   Name of club: `names`\n-   Link to club page: `urls`\n-   Type of club: `club_types`\n-   Resources of club: `resources`\n-   Membership type of club: `membership_types`\n-   Checkboxes that indicate the club is open / closed for joining: `membership_status`\n\nThe scheme shown in the image should also help you figure out which pieces of information come from which parts of the website.\n\n![](images/clubs-scrape-scheme.png){fig-align=\"center\" width=\"1000\"}\n\nYou will scrape each of these as individual vectors, clean them up a bit, and then bring them together in a data frame (a `tibble`) and write it out to a CSV file called `clubs.csv` in the `data` folder of your project/repository.\n\n::: callout-tip\n## Hint\n\nThere are some hints for cleaning the data in the script provided for you.\nFor example, some of the fields you scrape will have some extraneous characters in them like `\\n`s, `\\r`s, etc.\nTwo examples are provided below.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nname <- \"\\r Academic Advising Center\\r\"\nurl <- \"\\r\\n\\t\\t\\t\\t\\t\\t\\t\\thttps://dukegroups.com/AOTA/\\r\\n\\t\\t\\t\\t\\t\\t\\t\"\n```\n:::\n\n\n\n\nTo clean these up, first, we would want to *remove* the extraneous characters.\nThe **stringr** package (which comes with the tidyverse) offers some great functions for dealing with such nuisance, see <https://stringr.tidyverse.org/reference/index.html> for a full list.\nIn this case, we can use the following steps:\n\n-   `str_remove()` to remove the character(s) once or `str_remove_all()` to remove all occurences of the character(s) we don't want\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nname |>\n  str_remove_all(\"\\r\")\nurl |>\n  str_remove_all(\"\\r\") |>\n  str_remove_all(\"\\n\") |>\n  str_remove_all(\"\\t\")\n```\n:::\n\n\n\n\n-   `str_trim()` for trimming white space at the beginning and end of text strings\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nname |>\n  str_remove_all(\"\\r\") |>\n  str_trim()\n```\n:::\n\n\n\n\nThese types of functions will be helpful for cleaning the data you scrape as well.\n:::\n\n## Part 2 - Data cleaning\n\n### Question 2\n\nRead the `clubs.csv` file you created in Question 1 into your Quarto document.\n\nThe `club_type_detail` variable contains two pieces of information for most (though not all) clubs:\n\n-   Many of the entries are of the form `House/Quad Council - Housing & Residence Life` or `Open Membership (DSG) - Faith, Religion, & Spirituality`, where the piece before the hyphen (`-`) is `club_type` while the piece after the hyphen is `detail`.\n-   Some of the entries are of the form `Student Government` or `Student Affairs Unit`, basically just `club_type` but no `detail`.\n\nSeparate the `club_type` variable into two, `club_type` and `detail`, and then trim any extraneous white space from the resulting variables, as needed.\nSave the resulting dataset with these two new variables as `clubs`, i.e., overwrite the data frame.\n\nThen, display the first 10 rows of the dataset, `relocate()`ing `club_type` and `detail` variables to the beginning of the dataset to make sure they appear in the output in your rendered document.\n\n### Question 3\n\nAnother variable that needs some cleaning up is `membership_status`.\nCurrently it should either be `NA` or contain some text that says `\"Select ... to register for this group\"`.\nRecode this variable to say `\"Closed\"` if the current value is `NA` or `\"Open\"` otherwise.\nSave the resulting dataset with these two new variables as `clubs`, i.e., overwrite the data frame.\n\nThen, display the first 10 rows of the dataset, `relocate()`ing `membership_status` to the beginning of the dataset to make sure it appears in the output in your rendered document.\n\n::: callout-warning\nAt this point you should have a `clubs` dataset with seven columns in the following order:\n\n1.  `name`\n2.  `resources`\n3.  `club_type`\n4.  `detail`: May be blank for some clubs\n5.  `membership_type`\n6.  `membership_status`: with values `Open` and `Closed`\n7.  `url`\n\nIf your dataset has a structure of columns different than what's outlined above, go back to earlier exercises and review your answers.\n:::\n\n## Part 3 - Data analysis\n\n### Question 4\n\nSolve each of the following questions (parts) with a single pipeline.\n\na.  Find the distinct `membership_type`s and the number of clubs with each type of membership.\nb.  Find the number of clubs that have a Mission statement.\nc.  Find the clubs that are `Greek` and mention `Faith` in their detail.\n\n### Question 5\n\nRecreate the following table in a single pipeline.\n\n![](images/clubs-table.png){fig-align=\"center\" width=\"400\"}\n\n::: callout-tip\n## Hint\n\n-   First, count the number of clubs that fall into each `club_type` and `membership_status`.\n\n-   Then, pivot the resulting data frame so the levels of `membership_status` are across two columns as opposed to in a single column.\n\n-   Read the documentation for the pivoting function you're using to discover how you can replace `NA`s with 0s while pivoting the data.\n\n-   Create a new column `Total` that is the sum of `Closed` and `Open` columns.\n\n-   Use the `kable()` function (refer back to your take-home exam if you need a hint) to nicely format your table.\n:::\n\n### Question 6\n\nAsk a *simple* question of interest to you about student clubs at Duke that you can answer with these data.\nThen, answer your question in a single pipeline and a brief narrative describing your findings.\nIf your question is too complex to answer in a single pipeline, go back and revise your question.\n\n### Question 7\n\nAsk another question of interest to you about student clubs at Duke that you can answer with a visualization of these data.\nThen, answer your question, with a visualization and a brief narrative describing your findings.\n\n::: callout-note\nIf you are collaborating with classmates and brainstorming possible questions together, make sure the questions you choose are unique to you.\n:::\n\n## Part 4 - Ethics\n\n::: callout-important\nThe following two questions ask you to summarize articles on data science ethics.\nYou are not allowed to use Chat GPT or similar tools in answering these questions – you must actually read the articles and summarize them yourself.\nPlease be ethical, at a minimum when discussing ethics!\n:::\n\n### Question 8\n\nFor each of the following websites, first determine whether you're allowed to scrape data from them using tools we've learned in this course.\n\nThen, read (the relevant portions of their) Terms of Use/Service.\n\n-   ESPN: [https://www.espn.com](https://www.espn.com/) / <https://disneytermsofuse.com/english/#License-Grant-and-Restrictions>\n-   X/Twitter: [https://twitter.com](https://twitter.com/) / <https://twitter.com/en/tos>\n-   Rotten Tomatoes: [https://www.rottentomatoes.com](https://www.rottentomatoes.com/) / <https://www.rottentomatoes.com/policies/terms-of-use>\n\nFinally, summarize your findings about whether you can or cannot scrape data from these websites in 1 sentence for each website.\nAdditionally, quote the relevant sentence(s) from the Terms of Use/Service.\n\n::: callout-tip\n## Hint\n\nIn the Terms of Use/Service documents, it might be productive to search for keywords like \"scrape\" or \"scraping\" to find the relevant portions.\n:::\n\n### Question 9\n\nOne current ethical discussion in data science involves the training of \"Large Language Models\" such as ChatGPT.\nThese models are trained using massive corpora (document sets) that include large amounts of work that is covered under copyright law.\nRead the following two articles:\n\n-   [Do Large Language Models Violate Copyright Law?](https://www.dykema.com/news-insights/do-large-language-models-violate-copyright-law.html)\n-   [Reexamining \"Fair Use\" in the Age of AI](https://hai.stanford.edu/news/reexamining-fair-use-age-ai)\n\nWrite a short paragraph (maximum 8 sentences) discussing the arguments on both sides of the discussion over copyright in training large language models.\n\n### Question 10\n\nAnother major ethical discussion in data science resolves around discriminatory biases in machine learning models.\nThese biases can have real-world impacts in lending, criminal justice, hiring, and more.\nMany of these algorithms are so-called “black boxes”, meaning the exact process they take from input to output is unclear.\nRead the following articles:\n\n-   [Amazon scraps secret AI recruiting tool that showed bias against](https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html)\n-   [The Atlantic: The False Promise of Risk Assessment](https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/)\n\nWrite a short paragraph (maximum 8 sentences) discussing the nature of biases in machine learning and in datasets, and any possible solutions that could help limit those biases.\n\n# Wrap-up\n\n## Submission\n\nOnce you are finished with the lab, you will submit your final PDF document to Gradescope.\n\n::: callout-warning\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo.\nWe will be checking these to make sure you have been practicing how to commit and push changes.\n\nYou must turn in a PDF file to the Gradescope page by the submission deadline to be considered \"on time\".\n:::\n\nTo submit your assignment:\n\n-   Go to <http://www.gradescope.com> and click *Log in* in the top right corner.\n-   Click *School Credentials* $\\rightarrow$ *Duke NetID* and log in using your NetID credentials.\n-   Click on your *STA 199* course.\n-   Click on the assignment, and you'll be prompted to submit it.\n-   Mark all the pages associated with question. All the pages of your lab should be associated with at least one question (i.e., should be \"checked\").\n\n::: callout-important\n## Checklist\n\nMake sure you have:\n\n-   attempted all questions\n-   rendered your Quarto document\n-   committed and pushed everything to your GitHub repository such that the Git pane in RStudio is empty\n-   uploaded your PDF to Gradescope\n-   selected pages associated with each question on Gradescope\n:::\n\n## Grading\n\nThe lab is graded out of a total of 50 points.\n\nYou can earn up to 5 points on each question:\n\n-   5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n\n-   4: Response shows good understanding and addresses most of the rubric items.\n\n-   3: Response shows understanding and addresses a majority of the rubric items.\n\n-   2: Response shows effort and misses many of the rubric items.\n\n-   1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n\n-   0: No attempt.\n",
    "supporting": [
      "lab-4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}